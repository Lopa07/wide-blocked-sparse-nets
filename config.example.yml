### Required variables to train wide blocked sparse nets in PyTorch.

# Model parameters
model:
    # Model name to train. choices are "ResNet18"
    name: "ResNet18"

# Sparsity parameters
sparsity:
    # Base width
    base_width: 64

    # Widening factor
    widening_factor: 1.5

    # Layer types to sparsify
    layer_types:
        - "Linear"
        - "Conv2d"

    # Sparsity distribution type along layers. Choices are "large_to_small" and
    # "match_base_dist"
    dist_type: "large_to_small"

    # Sparsity pattern within a layer. Choices are "random", "io_only", and
    # "block"
    pattern: "random"

# Data parameters
dataset:
    # Dataset name to classify. Choices are "CIFAR10", "CIFAR100", "SVHN",
    # "MNIST", "FashionMNIST"
    name: "CIFAR10"

# Random seed for reproducibility
seed:

# Training parameters
training:
    # Number of epochs
    num_epochs: 200

    # Optimizer
    optimizer:
        # Optimizer name. Choices are: "sgd", "nesterov_sgd", "rmsprop", "adagrad",
        # or "adam"
        name: "sgd"
        # Initial learning rate
        learning_rate: 0.1
        # Momentum factor
        momentum: 0.9
        # Weight decay (L2 penalty)
        weight_decay: 0.0005

    # Scheduler
    scheduler:
        # Scheduler name. Choices are: "constant", "step", "multistep", "exponential",
        # or "cosine"
        name: "cosine"
        # Scheduler specific key word arguments
        kwargs:

    # Batch sizes
    batch_size:
        train: 128
        val: 100
